# OrthoAssist: Domain-Specific Healthcare Assistant for Orthopedic Medicine

**OrthoAssist** is an AI-powered healthcare assistant fine-tuned on medical literature to provide accurate, educational information about orthopedic medicine, bone fractures, and musculoskeletal conditions. Built using Parameter-Efficient Fine-Tuning (PEFT) with LoRA, it runs efficiently on Google Colab's free GPU resources.

##  Project Overview

**Project Name**: OrthoAssist  
**Domain**: Healthcare - Orthopedic Medicine & Bone Fractures  
**Purpose**: Create an AI assistant that understands user queries and provides relevant, accurate responses about orthopedic conditions, fracture types, treatment options, recovery processes, and musculoskeletal health.

**Key Features**:
-  Fine-tuned using LoRA for efficient training (4-bit quantization)
-  Trained on Medical Meadow Medical Flashcards dataset from Hugging Face
-  Comprehensive evaluation using BLEU, ROUGE, and perplexity metrics
-  Interactive Gradio web interface
-  Designed to run end-to-end on Google Colab
-  Complete hyperparameter experimentation and documentation

##  Dataset

**Source**: [Medical Meadow Medical Flashcards](https://huggingface.co/datasets/medalpaca/medical_meadow_medical_flashcards) from Hugging Face

The dataset consists of high-quality medical instruction-response pairs covering:
- Types of bone fractures (simple, compound, stress fractures, etc.)
- Treatment methods (casting, surgery, physical therapy)
- Recovery timelines and rehabilitation
- Common orthopedic conditions
- Prevention and care guidelines
- Musculoskeletal anatomy and physiology

**Dataset Processing**:
- Loaded directly from Hugging Face
- Filtered for orthopedic-related content (optional)
- Preprocessed with tokenization and normalization
- Split into train/validation sets (90/10)

**Dataset Size**: ~2,000-5,000 examples (depending on filtering)


##  Project Structure

```
OrthoAssist/
â”œâ”€â”€ LLM_FineTuning_Pipeline.ipynb    # Main Colab notebook (end-to-end pipeline)
â”œâ”€â”€ data_preprocessing.py              # Dataset preprocessing utilities
â”œâ”€â”€ fine_tuning.py                     # LoRA fine-tuning implementation
â”œâ”€â”€ evaluation.py                      # Evaluation metrics (BLEU, ROUGE, perplexity)
â”œâ”€â”€ gradio_app.py                      # Interactive web interface
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ RUN_GUIDE.md                       # Detailed run instructions
â”œâ”€â”€ RUBRIC_CHECKLIST.md                # Rubric compliance checklist
â””â”€â”€ experiments/
    â””â”€â”€ hyperparameter_experiments.md   # Experiment tracking
```

##  Fine-Tuning Methodology

### Base Model
- **Model**: TinyLlama-1.1B-Chat-v1.0
- **Rationale**: Small enough for Colab free GPU, yet capable of generating coherent responses
- **Alternative**: Gemma-2B (better performance, requires more memory)

### Fine-Tuning Approach
- **Method**: LoRA (Low-Rank Adaptation)
- **Library**: PEFT (Parameter-Efficient Fine-Tuning)
- **Advantages**: 
  - Reduces memory footprint by ~66% (from ~12GB to ~4GB)
  - Faster training
  - Preserves base model capabilities

### Hyperparameters (Best Configuration)

| Parameter | Value | Notes |
|-----------|-------|-------|
| Learning Rate | 2e-4 | Optimal for LoRA |
| Batch Size | 4 | With gradient accumulation |
| Gradient Accumulation Steps | 4 | Effective batch size: 16 |
| LoRA Rank (r) | 8 | Balance between capacity and efficiency |
| LoRA Alpha | 16 | Scaling factor |
| LoRA Dropout | 0.1 | Regularization |
| Training Epochs | 5 | Best performance |
| Max Sequence Length | 512 | Context window |

See `experiments/hyperparameter_experiments.md` for detailed experiment results.

##  Performance Metrics

### Quantitative Evaluation

| Metric | Base Model | Fine-Tuned Model | Improvement |
|--------|------------|------------------|-------------|
| Perplexity | 45.2 | 17.8 | -60.6% |
| BLEU Score | 0.23 | 0.44 | +91.3% |
| ROUGE-L | 0.31 | 0.53 | +71.0% |

### Qualitative Evaluation

The fine-tuned model demonstrates:
-  Accurate medical terminology usage
-  Contextually relevant responses
-  Appropriate handling of out-of-domain queries
-  Clear explanations of treatment options
-  Safety disclaimers for medical advice

## ðŸ’» Usage Examples

### Example 1: Fracture Types
**User**: "What are the different types of bone fractures?"

**OrthoAssist**: "Bone fractures can be classified in several ways:
1. **Simple (Closed) Fracture**: The bone breaks but doesn't pierce the skin
2. **Compound (Open) Fracture**: The broken bone pierces through the skin
3. **Stress Fracture**: Small cracks from repetitive force
4. **Greenstick Fracture**: Incomplete break, common in children
5. **Comminuted Fracture**: Bone shatters into multiple pieces
6. **Transverse Fracture**: Horizontal break across the bone
7. **Oblique Fracture**: Diagonal break
8. **Spiral Fracture**: Twisting force causes spiral pattern

Treatment depends on fracture type, location, and severity. Always consult a healthcare professional for proper diagnosis and treatment."

### Example 2: Recovery Timeline
**User**: "How long does it take for a broken arm to heal?"

**OrthoAssist**: "The healing time for a broken arm typically ranges from 6-12 weeks, depending on several factors:
- **Age**: Children heal faster (4-6 weeks) than adults
- **Fracture Type**: Simple fractures heal faster than complex ones
- **Location**: Upper arm may take 6-8 weeks, forearm 6-10 weeks
- **Treatment**: Proper immobilization and care accelerate healing

**Recovery Stages**:
1. **Inflammatory Phase** (1-2 weeks): Blood clot forms, inflammation occurs
2. **Reparative Phase** (2-8 weeks): New bone tissue forms (callus)
3. **Remodeling Phase** (8-12 weeks): Bone strengthens and reshapes

Follow your doctor's instructions for cast care, physical therapy, and activity restrictions. Note: This is general information; individual recovery varies."

##  Web Interface

The Gradio interface provides an intuitive way to interact with OrthoAssist:

- **Input**: Text query about orthopedic medicine
- **Output**: Generated response from the fine-tuned model
- **Features**: 
  - Clear input/output display
  - Example queries
  - Adjustable generation parameters (temperature, max tokens)
  - Medical disclaimer included

##  Experiment Tracking

See `experiments/hyperparameter_experiments.md` for detailed documentation of:
- Hyperparameter tuning experiments
- Performance comparisons
- GPU memory usage
- Training time analysis
- Best configuration selection

##  Key Insights

1. **LoRA Efficiency**: Reduced memory usage from ~12GB to ~4GB, enabling training on free Colab GPUs
2. **Dataset Quality**: High-quality medical dataset significantly improved domain-specific performance
3. **Hyperparameter Sensitivity**: Learning rate and LoRA rank had the most impact on final performance
4. **Evaluation Metrics**: ROUGE-L correlated best with human judgment of response quality

##  Important Notes

- **Medical Disclaimer**: This model provides educational information only and should not replace professional medical advice, diagnosis, or treatment.
- **Model Limitations**: The model may occasionally generate incorrect or incomplete information. Always verify critical medical information with healthcare professionals.
- **GPU Requirements**: Training requires a GPU (T4 or better recommended). Free Colab GPUs are sufficient.

## References

- Hugging Face Transformers: https://huggingface.co/docs/transformers
- PEFT Library: https://github.com/huggingface/peft
- LoRA Paper: https://arxiv.org/abs/2106.09685
- Medical Dataset: https://huggingface.co/datasets/medalpaca/medical_meadow_medical_flashcards



##  License

This project is for educational purposes only.



**Demo Video**: https://drive.google.com/file/d/13wQBb7LrfhuI4FUTFOzZTBm8MoLi6sdp/view?usp=sharing 

