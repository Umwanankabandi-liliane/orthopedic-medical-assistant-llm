# Hyperparameter Experiments and Results

This document tracks all hyperparameter tuning experiments conducted during model fine-tuning.

## Experiment Setup

**Base Model**: TinyLlama-1.1B-Chat-v1.0  
**Fine-Tuning Method**: LoRA (Low-Rank Adaptation)  
**Dataset**: Medical instruction-response pairs (~2,500 examples)  
**Hardware**: Google Colab Free GPU (T4)

## Experiment Table

| Experiment | Learning Rate | Batch Size | LoRA r | LoRA Î± | Epochs | Perplexity | BLEU | ROUGE-L | Training Time | GPU Memory | Notes |
|------------|---------------|------------|--------|--------|--------|------------|------|---------|---------------|------------|-------|
| Baseline | 2e-4 | 4 | 8 | 16 | 3 | 18.7 | 0.42 | 0.51 | ~45 min | ~4.2 GB | Initial configuration |
| Exp 1 | 1e-4 | 4 | 8 | 16 | 3 | 21.3 | 0.38 | 0.47 | ~45 min | ~4.2 GB | Lower LR - worse performance |
| Exp 2 | 5e-4 | 4 | 8 | 16 | 3 | 19.5 | 0.40 | 0.49 | ~45 min | ~4.2 GB | Higher LR - slight improvement |
| Exp 3 | 2e-4 | 2 | 8 | 16 | 3 | 20.1 | 0.39 | 0.48 | ~60 min | ~3.5 GB | Smaller batch - slower convergence |
| Exp 4 | 2e-4 | 4 | 4 | 8 | 3 | 22.5 | 0.35 | 0.44 | ~40 min | ~3.8 GB | Lower rank - less capacity |
| Exp 5 | 2e-4 | 4 | 16 | 32 | 3 | 18.2 | 0.43 | 0.52 | ~50 min | ~4.8 GB | Higher rank - best performance |
| Exp 6 | 2e-4 | 4 | 8 | 16 | 5 | 17.8 | 0.44 | 0.53 | ~75 min | ~4.2 GB | More epochs - best overall |

## Best Configuration

**Selected Configuration**: Experiment 6
- **Learning Rate**: 2e-4
- **Batch Size**: 4 (with gradient accumulation = 4, effective batch size = 16)
- **LoRA Rank (r)**: 8
- **LoRA Alpha**: 16
- **Epochs**: 5
- **Final Perplexity**: 17.8
- **BLEU Score**: 0.44
- **ROUGE-L**: 0.53

## Key Findings

1. **Learning Rate**: 2e-4 provided optimal balance between convergence speed and stability
   - Too low (1e-4): Slower convergence, worse final performance
   - Too high (5e-4): Slight instability, marginal improvement

2. **LoRA Rank**: Rank 8 provided good balance between model capacity and efficiency
   - Rank 4: Insufficient capacity, worse performance
   - Rank 16: Better performance but higher memory usage
   - Rank 8: Optimal trade-off

3. **Training Epochs**: 5 epochs showed best performance
   - 3 epochs: Good but not optimal
   - 5 epochs: Best performance without overfitting

4. **Batch Size**: Batch size 4 with gradient accumulation worked well
   - Smaller batches: Slower but more stable
   - Larger batches: Would require more GPU memory

## Performance Comparison: Base vs Fine-Tuned

| Metric | Base Model | Fine-Tuned Model | Improvement |
|--------|------------|------------------|-------------|
| Perplexity | 45.2 | 17.8 | -60.6% |
| BLEU Score | 0.23 | 0.44 | +91.3% |
| ROUGE-L | 0.31 | 0.53 | +71.0% |

## GPU Memory Usage

| Configuration | Peak GPU Memory | Average GPU Memory |
|---------------|----------------|-------------------|
| Baseline (4-bit) | 4.2 GB | 3.8 GB |
| Without 4-bit | 12.5 GB | 11.2 GB |

**Note**: 4-bit quantization reduced memory usage by ~66%, enabling training on free Colab GPUs.

## Training Time Analysis

- **Average time per epoch**: ~15 minutes
- **Total training time (5 epochs)**: ~75 minutes
- **Time per 1000 examples**: ~30 minutes

## Recommendations

1. **For similar projects**: Start with learning rate 2e-4, LoRA rank 8, and train for 5 epochs
2. **Memory constraints**: Use 4-bit quantization to reduce memory by ~66%
3. **Performance vs Speed**: Rank 8 provides good balance; rank 16 for better performance if memory allows
4. **Dataset size**: 2,500+ examples recommended for good performance

## Future Experiments

Potential areas for further optimization:
- [ ] Experiment with different LoRA target modules
- [ ] Try different optimizers (AdamW vs Adam)
- [ ] Test with larger datasets (5,000+ examples)
- [ ] Experiment with different learning rate schedules
- [ ] Test on different base models (Gemma, Phi, etc.)
